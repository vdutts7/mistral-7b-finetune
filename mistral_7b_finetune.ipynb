{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "390sC684Lnj-"
      },
      "source": [
        "# 1. Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYpSoOm3HHmZ"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/mistralai/mistral-finetune.git\n",
        "\n",
        "!pip install -r /content/mistral-finetune/requirements.txt\n",
        "\n",
        "# Download model + set directory\n",
        "!wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar\n",
        "!DIR=/content/mistral_models && mkdir -p $DIR && tar -xf mistral-7B-v0.3.tar -C $DIR\n",
        "\n",
        "\n",
        "\n",
        "#_______________Optional___________________________________\n",
        "# Alternatively, you can download the model from Hugging Face\n",
        "\n",
        "# !pip install huggingface_hub\n",
        "# from huggingface_hub import snapshot_download\n",
        "# from pathlib import Path\n",
        "\n",
        "# mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')\n",
        "# mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# snapshot_download(repo_id=\"mistralai/Mistral-7B-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n",
        "\n",
        "#! cp -r /root/mistral_models/7B-v0.3 /content/mistral_models\n",
        "#! rm -r /root/mistral_models/7B-v0.3\n",
        "\n",
        "!ls /content/mistral_models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MWEfE4jLvdM"
      },
      "source": [
        "# 2. Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHATpHaNSwz1",
        "outputId": "2a7fc142-be4e-4426-e753-ab800670f5b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbtB8TiQSxWa"
      },
      "outputs": [],
      "source": [
        "# make a new directory called data\n",
        "!mkdir -p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RS8xD62SzM_",
        "outputId": "ae9ac728-ff76-4022-b8d1-3b0e87fad50c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/data\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# navigate to this data directory\n",
        "%cd /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0HnziGHLCh4",
        "outputId": "b1a08dc0-85f6-430f-cde7-0fe605b72e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ultrachat_chunk_eval.jsonl  ultrachat_chunk_train.jsonl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# read data into a pandas dataframe\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet('https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k/resolve/main/data/test_gen-00000-of-00001-3d4cd8309148a71f.parquet')\n",
        "\n",
        "# split data into training and evaluation\n",
        "df_train=df.sample(frac=0.95,random_state=200)\n",
        "df_eval=df.drop(df_train.index)\n",
        "\n",
        "# save data into .jsonl files\n",
        "df_train.to_json(\"ultrachat_chunk_train.jsonl\", orient=\"records\", lines=True)\n",
        "df_eval.to_json(\"ultrachat_chunk_eval.jsonl\", orient=\"records\", lines=True)\n",
        "\n",
        "\n",
        "!ls /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rY0D8JLgD9d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# navigate to the mistral-finetune directory\n",
        "%cd /content/mistral-finetune/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# some of the training data doesn't have the right format,\n",
        "# so we need to reformat the data into the correct format and skip the cases that doesn't have the right format:\n",
        "\n",
        "!python -m utils.reformat_data /content/data/ultrachat_chunk_train.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eval data looking good\n",
        "!python -m utils.reformat_data /content/data/ultrachat_chunk_eval.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now you can verify your training yaml to make sure the data is correctly formatted and to get an estimate of your training time.\n",
        "\n",
        "!python -m utils.validate_data --train_yaml example/7B.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# these info is needed for training\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define training configuration\n",
        "# for your own use cases, you might want to change the data paths, model path, run_dir, and other hyperparameters\n",
        "\n",
        "config = \"\"\"\n",
        "# data\n",
        "data:\n",
        "  instruct_data: \"/content/data/ultrachat_chunk_train.jsonl\"  # Fill\n",
        "  data: \"\"  # Optionally fill with pretraining data\n",
        "  eval_instruct_data: \"/content/data/ultrachat_chunk_eval.jsonl\"  # Optionally fill\n",
        "\n",
        "# model\n",
        "model_id_or_path: \"/content/mistral_models\"  # Change to downloaded path\n",
        "lora:\n",
        "  rank: 64\n",
        "\n",
        "# optim\n",
        "# tokens per training steps = batch_size x num_GPUs x seq_len\n",
        "# we recommend sequence lentgh of 32768\n",
        "# If you run into memory error, you can try reduce the sequence length\n",
        "seq_len: 5120\n",
        "batch_size: 1\n",
        "num_microbatches: 8\n",
        "max_steps: 100\n",
        "optim:\n",
        "  lr: 1.e-4\n",
        "  weight_decay: 0.1\n",
        "  pct_start: 0.05\n",
        "\n",
        "# other\n",
        "seed: 0\n",
        "log_freq: 1\n",
        "eval_freq: 100\n",
        "no_eval: False\n",
        "ckpt_freq: 100\n",
        "\n",
        "save_adapters: True  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model\n",
        "\n",
        "run_dir: \"/content/test_ultra\"  # Fill\n",
        "\"\"\"\n",
        "\n",
        "# save the same file locally into the example.yaml file\n",
        "import yaml\n",
        "with open('example.yaml', 'w') as file:\n",
        "    yaml.dump(yaml.safe_load(config), file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! rm -r /content/test_ultra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# start training\n",
        "\n",
        "!torchrun --nproc-per-node 1 -m train example.yaml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
